model:
  encoder:
    class_path: mini_mask_git.modules.TransformerSequenceEncoder
    init_args:
      embed_dim: 512
      dim_model: 512
      num_layers: 12
      nhead: 8
      dim_feedforward: 1024
      activation: gelu
      max_len: 1024
      dropout: 0.0
      vocab_size: 1